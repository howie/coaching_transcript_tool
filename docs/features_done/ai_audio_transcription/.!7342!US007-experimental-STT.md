# US007: Experimental STT Configuration & Speaker Diarization Analysis

## Overview

This document analyzes the current Google Speech-to-Text v2 implementation challenges and proposes solutions for speaker diarization in coaching session transcriptions.

## Current Implementation Status

###  Step 1: Basic Transcription (COMPLETED)
- **Method**: Google STT v2 `batchRecognize`
- **Configuration**: 
  - Language: `cmn-Hant-TW` (BCP-47 format)
  - Model: `chirp_2` (latest with Chinese support)
  - Location: `asia-southeast1` (optimized for Asian languages)
  - Features: Auto punctuation, word timestamps, confidence scores
- **Result**: High-quality Chinese transcription without speaker diarization

### L Step 2: Speaker Diarization (NOT IMPLEMENTED)

## Technical Challenge: batchRecognize Limitation

### Problem
Google STT v2 speaker diarization is marked as "Preview" and only supports:
- `recognize` (synchronous, max 1 minute)
- `streamingRecognize` (real-time streaming)

**NOT supported**: `batchRecognize` (batch processing for long files)

### Error Encountered
```
400 Config contains unsupported fields. 
field_violations { field: "features.diarization_config" description: "Recognizer does not support feature: speaker_diarization" }
```

## Solution Architecture: Two-Stage Processing

### Stage 1: Transcription via batchRecognize 
- **Input**: Long audio file (up to 2 hours)
- **Output**: Word-level transcription with timestamps
- **Benefits**:
  -  Handles long coaching sessions (30-120 minutes)
  -  High accuracy for Chinese with `chirp_2` model
  -  Word-level timing information preserved
  -  Automatic punctuation and confidence scores

### Stage 2: Post-Processing Speaker Diarization L
- **Input**: Audio file + word-level transcript with timestamps
- **Output**: Speaker-segmented transcript
- **Implementation Options**:

## Speaker Diarization Implementation Options

### Option 1: PyAnnote.audio (Recommended)
```python
from pyannote.audio import Pipeline

pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
diarization = pipeline("audio.wav")
```

**Advantages**:
-  State-of-the-art accuracy
-  Supports Chinese and multilingual audio
-  Pre-trained models available
-  Active community and updates

**Disadvantages**:
- L Requires HuggingFace account and token
- L Additional compute resources needed
- L Extra processing time (30-60 seconds per minute of audio)

### Option 2: SpeechBrain Diarization
```python
from speechbrain.pretrained import SpeakerRecognition

model = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb", 
    savedir="pretrained_models/spkrec-ecapa-voxceleb"
)
```

**Advantages**:
-  Good accuracy
-  No external API dependencies
-  Integrated with SpeechBrain ecosystem

**Disadvantages**:
- L More complex setup
- L Less documentation for Chinese
- L Requires manual clustering implementation

### Option 3: Google STT v2 Streaming (Alternative)
Switch from `batchRecognize` to `streamingRecognize` with chunking.

**Advantages**:
-  Native diarization support
-  Same quality as batch processing
-  No additional ML models needed

**Disadvantages**:
- L Complex chunking logic for long files
- L Requires significant code restructuring
- L Streaming quota limits

## Use Cases & Benefits Analysis

### For Coaching Sessions

#### Primary Use Case: Coach-Client Identification
- **Benefit**: Automatic separation of coach vs. client speech
- **Value**: Enables role-based analysis and coaching effectiveness metrics
