"""
Manual verification script for LeMUR Chinese processing fixes.

This script allows quick testing of the improved LeMUR processing logic
with real-world examples from user logs.
"""

import asyncio
import logging
import sys
import os

# Add source path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../src'))

from coaching_assistant.services.lemur_transcript_smoother import (
    LeMURTranscriptSmoother, 
    SmoothingContext
)


# Setup logging to see detailed processing
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ManualLeMURTester:
    """Manual testing class for LeMUR processing improvements."""
    
    def __init__(self):
        self.smoother = LeMURTranscriptSmoother()
        self.context = SmoothingContext(session_language="zh-TW")
    
    def test_text_cleanup_functions(self):
        """Test the core text cleanup functions."""
        print("=" * 60)
        print("ğŸ§ª TESTING CORE TEXT CLEANUP FUNCTIONS")
        print("=" * 60)
        
        test_cases = [
            # Space removal test cases
            ("æˆ‘ æƒ³è¦ èŠ ä¸€ä¸ª å…³äº", "æˆ‘æƒ³è¦èŠä¸€ä¸ªå…³äº"),
            ("å—¯ å° ç¢ºå¯¦ å—¯ å¥½", "å—¯å°ç¢ºå¯¦å—¯å¥½"),
            ("åªæ˜¯ æƒ³ èªª é€™å€‹ å•é¡Œ", "åªæ˜¯æƒ³èªªé€™å€‹å•é¡Œ"),
            ("ä½ å¥½ ï¼Œ æˆ‘ æ˜¯", "ä½ å¥½ï¼Œæˆ‘æ˜¯"),
            ("LisaOK å— OK æˆ‘ æ²¡æœ‰ é—®é¢˜", "LisaOKå—OKæˆ‘æ²¡æœ‰é—®é¢˜"),
            
            # Mixed language cases
            ("Hello ä¸– ç•Œ æ¸¬ è©¦", "Hello ä¸–ç•Œæ¸¬è©¦"),
            ("This is a test é€™ æ˜¯ æ¸¬ è©¦", "This is a test é€™æ˜¯æ¸¬è©¦"),
            
            # Punctuation cases
            ("çœŸçš„ å— ï¼Ÿ", "çœŸçš„å—ï¼Ÿ"),
            ("å¤ªå¥½äº† ï¼ è¬è¬", "å¤ªå¥½äº†ï¼è¬è¬"),
        ]
        
        print("\nğŸ“ Testing _clean_chinese_text_spacing function:")
        all_passed = True
        
        for i, (input_text, expected) in enumerate(test_cases):
            result = self.smoother._clean_chinese_text_spacing(input_text)
            passed = result == expected
            all_passed &= passed
            
            status = "âœ… PASS" if passed else "âŒ FAIL"
            print(f"  {i+1:2d}. {status} '{input_text}' â†’ '{result}' (expected: '{expected}')")
            
            if not passed:
                print(f"      âš ï¸  Expected: '{expected}'")
                print(f"      âš ï¸  Got:      '{result}'")
        
        print(f"\nğŸ¯ Text Cleanup Results: {sum(1 for _, (i, e) in enumerate(test_cases) if self.smoother._clean_chinese_text_spacing(i) == e)}/{len(test_cases)} tests passed")
        return all_passed
    
    def test_mandatory_cleanup_function(self):
        """Test the mandatory cleanup function with Traditional Chinese conversion."""
        print("\n=" * 60)
        print("ğŸ”„ TESTING MANDATORY CLEANUP FUNCTION")
        print("=" * 60)
        
        test_cases = [
            # Simplified to Traditional conversion cases
            ("æˆ‘ æƒ³è¦ èŠ ä¸€ä¸ª å…³äº ç”Ÿæ´» å·¥ä½œ", "æˆ‘æƒ³è¦èŠä¸€å€‹é—œæ–¼ç”Ÿæ´»å·¥ä½œ"),
            ("å¥½ çš„ æ²¡æœ‰ é—®é¢˜", "å¥½çš„æ²’æœ‰å•é¡Œ"),
            ("è°¢è°¢ ä½  æ„¿æ„ åˆ†äº«", "è¬è¬ä½ é¡˜æ„åˆ†äº«"),
        ]
        
        print("\nğŸ“ Testing _apply_mandatory_cleanup function:")
        
        for i, (input_text, expected_contains) in enumerate(test_cases):
            try:
                result = self.smoother._apply_mandatory_cleanup(input_text, context_language="zh")
                print(f"  {i+1}. Input:  '{input_text}'")
                print(f"     Output: '{result}'")
                
                # Check key transformations
                has_no_extra_spaces = '  ' not in result and ' æƒ³' not in result
                has_traditional = any(char in result for char in ['å€‹', 'é—œ', 'æ²’', 'å•', 'é¡Œ', 'è¬'])
                
                print(f"     âœ… No extra spaces: {has_no_extra_spaces}")
                print(f"     âœ… Traditional Chinese: {has_traditional}")
                print()
                
            except Exception as e:
                print(f"  {i+1}. âŒ ERROR: {e}")
    
    def test_segment_merging(self):
        """Test segment merging functionality."""
        print("=" * 60)
        print("ğŸ”— TESTING SEGMENT MERGING")
        print("=" * 60)
        
        # Test case based on actual user log
        test_segments = [
            {'speaker': 'B', 'text': 'æˆ‘ä»¬ä»Šå¤©å°±å…ˆæš‚æ—¶ç»“æŸåœ¨è¿™è¾¹', 'start': 1785376, 'end': 1798619},
            {'speaker': 'B', 'text': 'LisaOKå—', 'start': 1799480, 'end': 1800480},  # Small gap
            {'speaker': 'B', 'text': 'OKæˆ‘æ²¡æœ‰é—®é¢˜', 'start': 1800800, 'end': 1802141},  # Small gap
            {'speaker': 'A', 'text': 'å¥½è°¢è°¢ä½ ', 'start': 1805000, 'end': 1806000},  # Different speaker
        ]
        
        print(f"ğŸ“¥ Original segments: {len(test_segments)}")
        for i, seg in enumerate(test_segments):
            gap = test_segments[i+1]['start'] - seg['end'] if i < len(test_segments)-1 else 0
            print(f"  {i+1}. Speaker {seg['speaker']}: '{seg['text']}' (gap to next: {gap}ms)")
        
        merged = self.smoother._merge_close_segments(test_segments, max_gap_ms=5000)  # Generous gap for this test
        
        print(f"\nğŸ“¤ Merged segments: {len(merged)}")
        for i, seg in enumerate(merged):
            print(f"  {i+1}. Speaker {seg['speaker']}: '{seg['text']}'")
        
        # Should merge the B segments
        b_segments = [seg for seg in merged if seg['speaker'] == 'B']
        if len(b_segments) == 1:
            print("âœ… Successfully merged fragmented B segments")
            print(f"   Combined text: '{b_segments[0]['text']}'")
        else:
            print(f"âš ï¸  Expected 1 B segment, got {len(b_segments)}")
    
    def test_response_parsing(self):
        """Test response parsing with various formats."""
        print("=" * 60)
        print("ğŸ“‹ TESTING RESPONSE PARSING")
        print("=" * 60)
        
        test_responses = [
            # Case 1: Pure text response
            {
                "name": "Pure Text Response",
                "response": """
                æ•™ç·´: æˆ‘æƒ³è¦èŠä¸€å€‹é—œæ–¼ç”Ÿæ´»å·¥ä½œå¹³è¡¡çš„è­°é¡Œã€‚
                å®¢æˆ¶: å¥½çš„ï¼Œæ²’æœ‰å•é¡Œï¼Œæˆ‘å€‘å¯ä»¥é–‹å§‹è¨è«–ã€‚
                æ•™ç·´: è¬è¬ä½ é¡˜æ„åˆ†äº«ã€‚
                """,
                "expected_segments": 3
            },
            
            # Case 2: Mixed format
            {
                "name": "Mixed JSON + Text Response", 
                "response": """
                {"A": "æ•™ç·´", "B": "å®¢æˆ¶"}
                
                æ•™ç·´: æˆ‘æƒ³è¦èŠä¸€å€‹é—œæ–¼å·¥ä½œçš„è­°é¡Œã€‚
                å®¢æˆ¶: å¥½çš„ï¼Œæ²’æœ‰å•é¡Œã€‚
                """,
                "expected_segments": 2
            },
            
            # Case 3: Malformed response
            {
                "name": "Malformed Response",
                "response": """
                Some random text here
                A: Content from A
                Random line without colon
                B: Content from B
                """,
                "expected_segments": 2
            }
        ]
        
        original_segments = [
            {'speaker': 'A', 'text': 'original', 'start': 1000, 'end': 2000},
            {'speaker': 'B', 'text': 'original', 'start': 2000, 'end': 3000},
            {'speaker': 'A', 'text': 'original', 'start': 3000, 'end': 4000},
        ]
        
        for test_case in test_responses:
            print(f"\nğŸ“ Testing: {test_case['name']}")
            print(f"Response preview: {test_case['response'][:100]}...")
            
            try:
                speaker_mapping, segments = self.smoother._parse_combined_response(
                    test_case['response'], original_segments, self.context
                )
                
                print(f"  âœ… Parsed {len(segments)} segments (expected ~{test_case['expected_segments']})")
                print(f"  ğŸ“Š Speaker mapping: {speaker_mapping}")
                
                if segments:
                    print("  ğŸ“¤ Sample segments:")
                    for i, seg in enumerate(segments[:2]):  # Show first 2
                        print(f"    {i+1}. {seg.speaker}: {seg.text[:50]}...")
                
            except Exception as e:
                print(f"  âŒ PARSING FAILED: {e}")
    
    async def test_full_pipeline_mock(self):
        """Test the full pipeline with mocked LeMUR."""
        print("=" * 60)
        print("ğŸš€ TESTING FULL PIPELINE (MOCK)")
        print("=" * 60)
        
        # Use the actual problematic segment from user logs
        test_segments = [{
            'speaker': 'B', 
            'text': 'å—¯ å° ç¢ºå¯¦ å—¯ å¥½ é‚£ æˆ‘ä»¬ ä»Šå¤© å°± å…ˆ æš‚æ—¶ ç»“æŸ åœ¨ è¿™è¾¹ LisaOK å— OK æˆ‘ æ²¡æœ‰ é—®é¢˜ å¥½ è°¢è°¢ ä½  è°¢è°¢ è°¢è°¢ å¥½.', 
            'start': 1785376, 
            'end': 1804782
        }]
        
        print(f"ğŸ“¥ Input segment:")
        print(f"   Speaker: {test_segments[0]['speaker']}")
        print(f"   Text: '{test_segments[0]['text']}'")
        print(f"   Issues: Spaces between Chinese, simplified Chinese, fragmented content")
        
        # Mock the LeMUR response processing directly
        mock_response = """
        å®¢æˆ¶: å—¯ï¼Œå°ç¢ºå¯¦å—¯å¥½ï¼Œé‚£æˆ‘å€‘ä»Šå¤©å°±å…ˆæš«æ™‚çµæŸåœ¨é€™é‚Šï¼ŒLisa OKå—ï¼ŸOKæˆ‘æ²’æœ‰å•é¡Œï¼Œå¥½è¬è¬ä½ è¬è¬è¬è¬å¥½ã€‚
        """
        
        print(f"\nğŸ“ Simulated LeMUR response:")
        print(f"   '{mock_response.strip()}'")
        
        # Test parsing
        try:
            speaker_mapping, segments = self.smoother._parse_combined_response(
                mock_response, test_segments, self.context
            )
            
            if segments:
                result_segment = segments[0]
                print(f"\nğŸ“¤ Processed result:")
                print(f"   Speaker: {result_segment.speaker}")
                print(f"   Text: '{result_segment.text}'")
                
                # Check improvements
                improvements = []
                if 'å—¯ å°' not in result_segment.text:
                    improvements.append("âœ… Removed spaces between Chinese")
                if 'æˆ‘å€‘' in result_segment.text:
                    improvements.append("âœ… Converted to Traditional Chinese")
                if result_segment.speaker in ['å®¢æˆ¶', 'æ•™ç·´']:
                    improvements.append("âœ… Correct speaker identification")
                
                print(f"\nğŸ¯ Improvements detected:")
                for improvement in improvements:
                    print(f"   {improvement}")
                    
        except Exception as e:
            print(f"âŒ PIPELINE TEST FAILED: {e}")


async def main():
    """Run all manual tests."""
    print("ğŸ§ª LeMUR Chinese Processing - Manual Verification")
    print("=" * 80)
    
    tester = ManualLeMURTester()
    
    try:
        # Test core functions
        print("\n1ï¸âƒ£ Testing Core Functions...")
        tester.test_text_cleanup_functions()
        tester.test_mandatory_cleanup_function()
        tester.test_segment_merging()
        tester.test_response_parsing()
        
        # Test full pipeline
        print("\n2ï¸âƒ£ Testing Full Pipeline...")
        await tester.test_full_pipeline_mock()
        
        print("\n" + "=" * 80)
        print("âœ… MANUAL VERIFICATION COMPLETED")
        print("ğŸ“‹ Review the results above to verify improvements are working correctly.")
        print("ğŸ”„ To test with real LeMUR API, use the E2E test scripts.")
        
    except Exception as e:
        print(f"\nâŒ MANUAL TESTING FAILED: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())